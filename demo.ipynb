{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shida/anaconda3/envs/s4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from s4 import S4Block as S4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperparametersConfig:\n",
    "    seed: int = 0\n",
    "    ## Data\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    ## Model\n",
    "    d_model: int = 128\n",
    "    n_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "    prenorm: bool = True\n",
    "    ## Optimization\n",
    "    lr: float = 0.01\n",
    "    weight_decay: float = 0.01\n",
    "    epochs: int = 100\n",
    "\n",
    "config = HyperparametersConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(1, 784).t())\n",
    "])\n",
    "transform_train = transform_test = transform\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "valset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_test)\n",
    "_, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "d_input = 1\n",
    "d_output = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "trainset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "valset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "testset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=d_input, hidden_size=128, num_layers=1, batch_first=True)\n",
    "lstm = nn.LSTM(input_size=d_input, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4(d_model, dropout=dropout, transposed=True, lr=min(0.001, config.lr))\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(nn.Dropout1d(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "s4 = S4Model(\n",
    "    d_input=d_input,\n",
    "    d_output=d_output,\n",
    "    d_model=config.d_model,\n",
    "    n_layers=config.n_layers,\n",
    "    dropout=config.dropout,\n",
    "    prenorm=config.prenorm,\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rnn = rnn.to(device)\n",
    "lstm = lstm.to(device)\n",
    "s4 = s4.to(device)\n",
    "\n",
    "rnn_optimizer = optim.AdamW(rnn.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "lstm_optimizer = optim.AdamW(lstm.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "s4_optimizer = optim.AdamW(s4.parameters(), lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (304/422) | Loss: 0.494 | Acc: 83.120% (32450/39040): : 305it [00:20, 15.66it/s]"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, criterion, trainloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )\n",
    "    \n",
    "    return train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss, acc = train(s4, s4_optimizer, criterion, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RNN, LSTM, and S4 training curves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
