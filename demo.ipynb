{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shida/anaconda3/envs/s4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from s4 import S4Block as S4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperparametersConfig:\n",
    "    seed: int = 0\n",
    "    ## Data\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    ## Model\n",
    "    d_model: int = 128\n",
    "    n_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "    prenorm: bool = True\n",
    "    ## Optimization\n",
    "    lr: float = 0.01\n",
    "    weight_decay: float = 0.01\n",
    "    epochs: int = 10\n",
    "\n",
    "config = HyperparametersConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(1, 784).t())\n",
    "])\n",
    "transform_train = transform_test = transform\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "valset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_test)\n",
    "_, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "d_input = 1\n",
    "d_output = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=128,\n",
    "        n_layers=1,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(d_input, d_model, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x, _ = self.rnn(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=128,\n",
    "        n_layers=1,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(d_input, d_model, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x, _ = self.lstm(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=128,\n",
    "        n_layers=1,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(S4(d_model, dropout=dropout, transposed=False, lr=min(0.001, config.lr)))\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        for layer in self.s4_layers:\n",
    "            x, _ = self.s4(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN  model has 117130 parameters\n",
      "LSTM model has 464650 parameters\n",
      "S4   model has 265482 parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn = RNNModel(d_input=d_input,d_output=d_output,d_model=config.d_model,n_layers=config.n_layers,dropout=config.dropout,)\n",
    "lstm = LSTMModel(d_input=d_input,d_output=d_output,d_model=config.d_model,n_layers=config.n_layers,dropout=config.dropout,)\n",
    "s4 = S4Model(d_input=d_input,d_output=d_output,d_model=config.d_model,n_layers=config.n_layers,dropout=config.dropout)\n",
    "\n",
    "# print the number of parameters for each model\n",
    "print(\"RNN  model has\", sum(p.numel() for p in rnn.parameters()), \"parameters\")\n",
    "print(\"LSTM model has\", sum(p.numel() for p in lstm.parameters()), \"parameters\")\n",
    "print(\"S4   model has\", sum(p.numel() for p in s4.parameters()), \"parameters\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rnn = rnn.to(device)\n",
    "lstm = lstm.to(device)\n",
    "s4 = s4.to(device)\n",
    "\n",
    "rnn_optimizer = optim.AdamW(rnn.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "lstm_optimizer = optim.AdamW(lstm.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "s4_optimizer = optim.AdamW(s4.parameters(), lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (421/422) | Loss: 2.303 | Acc: 11.200% (6048/54000): : 422it [00:09, 46.01it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 94.01it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.302 | Acc: 11.350% (1135/10000): : 79it [00:00, 101.59it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.302 | Acc: 11.063% (5974/54000): : 422it [00:09, 42.90it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 84.86it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.302 | Acc: 11.350% (1135/10000): : 79it [00:00, 106.50it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.302 | Acc: 11.183% (6039/54000): : 422it [00:09, 44.08it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.302 | Acc: 11.100% (666/6000): : 47it [00:00, 98.79it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 101.68it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.302 | Acc: 11.207% (6052/54000): : 422it [00:09, 45.21it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.302 | Acc: 11.100% (666/6000): : 47it [00:00, 96.69it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 105.16it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.302 | Acc: 11.178% (6036/54000): : 422it [00:09, 45.21it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.302 | Acc: 11.100% (666/6000): : 47it [00:00, 95.46it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 110.43it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.302 | Acc: 11.252% (6076/54000): : 422it [00:09, 46.47it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 92.27it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 101.27it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.301 | Acc: 11.252% (6076/54000): : 422it [00:09, 46.38it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 99.36it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 107.35it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.301 | Acc: 11.252% (6076/54000): : 422it [00:09, 46.69it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 96.10it/s] \n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 106.60it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.301 | Acc: 11.252% (6076/54000): : 422it [00:09, 46.77it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 104.17it/s]\n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 109.19it/s]\n",
      "Batch Idx: (421/422) | Loss: 2.301 | Acc: 11.252% (6076/54000): : 422it [00:08, 46.93it/s]\n",
      "Batch Idx: (46/47) | Loss: 2.301 | Acc: 11.100% (666/6000): : 47it [00:00, 103.77it/s]\n",
      "Batch Idx: (78/79) | Loss: 2.301 | Acc: 11.350% (1135/10000): : 79it [00:00, 104.29it/s]\n",
      "Epoch: 9 | Val acc: 11.100: 100%|██████████| 10/10 [01:53<00:00, 11.36s/it]\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, criterion, trainloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )\n",
    "    \n",
    "    return model, train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "            )\n",
    "\n",
    "    return eval_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(rnn_optimizer, config.epochs)\n",
    "pbar = tqdm(range(0, config.epochs))\n",
    "rnn_train_acc = []\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    rnn, train_loss, train_acc = train(rnn, rnn_optimizer, criterion, trainloader)\n",
    "    val_loss, val_acc = eval(rnn, valloader)\n",
    "    test_loss, test_acc = eval(rnn, testloader)\n",
    "    rnn_scheduler.step()\n",
    "    # print(f\"Epoch {epoch} learning rate: {scheduler.get_last_lr()}\")\n",
    "    rnn_train_acc.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m lstm_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(\u001b[43mlstm_optimizer\u001b[49m, config\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m      3\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mepochs))\n\u001b[1;32m      4\u001b[0m lstm_train_acc \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lstm_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lstm_optimizer, config.epochs)\n",
    "pbar = tqdm(range(0, config.epochs))\n",
    "lstm_train_acc = []\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    lstm, train_loss, train_acc = train(lstm, lstm_optimizer, criterion, trainloader)\n",
    "    val_loss, val_acc = eval(lstm, valloader)\n",
    "    test_loss, test_acc = eval(lstm, testloader)\n",
    "    lstm_scheduler.step()\n",
    "    # print(f\"Epoch {epoch} learning rate: {scheduler.get_last_lr()}\")\n",
    "    lstm_train_acc.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "s4_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(s4_optimizer, config.epochs)\n",
    "pbar = tqdm(range(0, config.epochs))\n",
    "s4_train_acc = []\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    s4, train_loss, train_acc = train(s4, s4_optimizer, criterion, trainloader)\n",
    "    val_loss, val_acc = eval(s4, valloader)\n",
    "    test_loss, test_acc = eval(s4, testloader)\n",
    "    s4_scheduler.step()\n",
    "    # print(f\"Epoch {epoch} learning rate: {scheduler.get_last_lr()}\")\n",
    "    s4_train_acc.append(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RNN, LSTM, and S4 training curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rnn_train_acc, label='RNN')\n",
    "plt.plot(lstm_train_acc, label='LSTM')\n",
    "plt.plot(s4_train_acc, label='S4')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
